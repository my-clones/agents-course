# LLM

Um LLM √© um tipo de modelo de IA que se **destaca na compreens√£o e gera√ß√£o de linguagem humana**. 

A maioria dos LLMs atuais √© constru√≠da na arquitetura `Transformer` ‚Äî uma arquitetura de ***deep learning*** baseada no algoritmo `Attention`, que ganhou interesse significativo desde o lan√ßamento do BERT pelo Google em 2018.

---

## Transformes

Existem 3 tipos de `transformers`:

**1. Encoders**
Um `encoder-based transformer` recebe texto (ou outros dados) como entrada e gera uma representa√ß√£o densa (ou embedding/incorpora√ß√£o) desse texto.
* Exemplo: BERT do Google
* Casos de Uso: Classifica√ß√£o de texto, busca sem√¢ntica, Reconhecimento de Entidades Nomeadas
* Tamanho T√≠pico: Milh√µes de par√¢metros

**2. Decoders**
Um `decoder-based transformer` concentra-se na gera√ß√£o de novos tokens para completar uma sequ√™ncia, um token por vez.

* Exemplo: Llama do Meta
* Casos de Uso: Gera√ß√£o de texto, chatbots, gera√ß√£o de c√≥digo
* Tamanho T√≠pico: Bilh√µes (no sentido americano, ou seja, 10^9) de par√¢metros

**3. Seq2seq**
Um `sequence-to-sequence transformer` combina um ENCODER e um DECODER. 
O codificador primeiro processa a sequ√™ncia de entrada em uma representa√ß√£o de contexto e, em seguida, o decodificador gera uma sequ√™ncia de sa√≠da.

* Exemplo: T5, BART
* Casos de Uso: Tradu√ß√£o, Resumo, Par√°frase
* Tamanho T√≠pico: Milh√µes de par√¢metros

---

## O que um LLM faz?

> O princ√≠pio subjacente de um LLM √© simples, mas altamente eficaz: 
> seu objetivo √© **prever o pr√≥ximo `token`**, dada uma sequ√™ncia de `tokens` anteriores. 

* **Um `token` √© a unidade de informa√ß√£o com a qual um LLM trabalha.**
  * Voc√™ pode pensar em um `token` como se fosse uma "palavra", mas, por raz√µes de efici√™ncia, os LLMs n√£o usam palavras inteiras.
  * Por exemplo, enquanto o ingl√™s tem cerca de 600.000 palavras, um LLM pode ter um vocabul√°rio de cerca de 32.000 `tokens` (como √© o caso do Llama 2). 
* **A tokeniza√ß√£o geralmente funciona em unidades de `subpalavras` que podem ser combinadas.**
  * Por exemplo, considere como os **tokens** `"interest"` e `"ing"` podem ser combinados para formar `"interesting"`, ou como `"ed"` pode ser adicionado para formar `"interested"`.
* **Cada LLM tem alguns tokens especiais**
  * Por exemplo, um token de fim de sequ√™ncia (EOS) [considerado o mais importante].
  * Os formatos de tokens especiais s√£o bastante diversos entre os provedores de modelos.

---

### Compreendendo a previs√£o do pr√≥ximo token

Os LLMs s√£o considerados autorregressivos, o que significa: 
* que a sa√≠da de uma passagem se torna a entrada para a pr√≥xima. 
* esse loop continua at√© que o modelo alcance o token `EOS`. 
  
```mermaid
stateDiagram-v2
    direction LR
    [*] -->  GenerateToken : input
    state eos_check <<choice>>
    GenerateToken --> eos_check
    eos_check --> GenerateToken: if not EOS
    eos_check --> [*]: if EOS 
```

### Mas o que acontece durante um loop de decodifica√ß√£o?

Uma breve vis√£o geral:

* O texto de entrada √© tokenizado, 
* O modelo **calcula uma representa√ß√£o da sequ√™ncia**, o que captura informa√ß√µes sobre o significado e a posi√ß√£o de cada **token** na **sequ√™ncia de entrada**.
* √â calculado a probabilidade de cada token ser o pr√≥ximo na sequ√™ncia. 
* Com base nessas pontua√ß√µes, temos diversas estrat√©gias para selecionar os tokens que completam a frase.
  * A estrat√©gia de decodifica√ß√£o mais f√°cil seria sempre escolher o token com a pontua√ß√£o mais alta.
* Isso se repete at√© o modelo gerar o token `EOS`.


```mermaid
flowchart LR
    A[Input Text] --> B[Tokenized]
    B --> C[Compute context]
    C --> D[Compute Probabilities]
    D --> E[Decoding strategy]
```

---

### Decoding strategy

* **Greedy decoding**: Sempre escolha o token com a maior probabilidade.
* **Beam search**: Explora v√°rias sequ√™ncias candidatas e escolha aquela com a maior pontua√ß√£o total.
* **Sampling**: Amostragem aleat√≥ria de tokens da distribui√ß√£o de probabilidade.
* **Top-k sampling**: Amostragem aleat√≥ria de tokens entre os k tokens mais prov√°veis.

--- 

### Attention is all you need

> üß† Este processo de **identificar as palavras mais relevantes** para **prever o pr√≥ximo token** provou ser **incrivelmente eficaz**.

Um aspecto fundamental da arquitetura do Transformer √© a Aten√ß√£o. Ao prever a pr√≥xima palavra, nem todas as palavras em uma frase s√£o igualmente importantes; 

* Por exemplo: palavras como "Fran√ßa" e "capital" na frase "A capital da Fran√ßa √©..." s√£o as que carregam mais significado.

Embora o princ√≠pio b√°sico dos LLMs ‚Äî prever o pr√≥ximo token ‚Äî tenha permanecido consistente desde o GPT-2, **houve avan√ßos significativos no escalonamento de redes neurais e no funcionamento do mecanismo de aten√ß√£o por sequ√™ncias cada vez mais longas.**

Se voc√™ j√° interagiu com LLMs, provavelmente est√° familiarizado com o termo comprimento de contexto (`context length`), que se refere ao n√∫mero m√°ximo de tokens que o LLM pode processar e ao seu per√≠odo m√°ximo de aten√ß√£o.

---

### Prompting o LLM √© importante

Considerando que a √∫nica fun√ß√£o de um LLM √© prever o **pr√≥ximo token** analisando cada token de entrada e escolher quais tokens s√£o "importantes", a **formula√ß√£o da sua sequ√™ncia de entrada √© muito importante**.

> **prompt** √© a sequ√™ncia de entrada que voc√™ fornece a um LLM.

O **design do prompt** facilita o <u>trabalho do LLM</u> de gerar **sa√≠da desejada**.


### Como os LLMs s√£o treinados?

Os LLMs s√£o treinados em grandes conjuntos de dados de texto, onde aprendem a prever a pr√≥xima palavra em uma sequ√™ncia por meio de um objetivo de modelagem de linguagem autossupervisionado ou mascarado.

A partir desse aprendizado n√£o supervisionado, o modelo aprende a estrutura da linguagem e os padr√µes subjacentes no texto, permitindo que o modelo generalize para dados invis√≠veis.

Ap√≥s esse **pr√©-treinamento** inicial, os LLMs podem ser **ajustados** com base em um objetivo de aprendizado supervisionado **para executar tarefas espec√≠ficas**. 

* Por exemplo, alguns modelos s√£o treinados para
  * estruturas **conversacionais** ou **uso de ferramentas**, 
  * enquanto outros se concentram em **classifica√ß√£o** ou **gera√ß√£o de c√≥digo**.

---

### Como os LLMs s√£o usados em Agentes de IA?

> üß† LLM √© o c√©rebro do Agente.

Os `LLMs` s√£o um **componente essencial** dos `Agentes de IA`, fornecendo a b**ase para a compreens√£o e gera√ß√£o da linguagem humana**.

os `LLMs` podem interpretar instru√ß√µes do usu√°rio, manter o contexto em conversas, definir um plano e decidir quais ferramentas usar.

---

## Chat Templates (modelos de chat)

source: [HF:messages-and-special-tokens](https://huggingface.co/learn/agents-course/unit1/messages-and-special-tokens)


Cada LLM usa uma formata√ß√£o espec√≠fica de prompt para formatar as mensagens num formato que ele entenda.

* Assim como cada LLM usa seu pr√≥prio token `EOS` (Fim de Sequ√™ncia), 
* eles tamb√©m usam regras de formata√ß√£o e delimitadores diferentes para as mensagens na conversa.

Os **`modelos de chat`** atuam como a **ponte** entre as `mensagens` (os turnos do usu√°rio e do assistente) e os `requisitos de formata√ß√£o` espec√≠ficos do LLM escolhido. 
Nos bastidores, essas mensagens s√£o concatenadas e formatadas em um prompt que o modelo pode entender.

üí¨ Os **`modelos de chat`** estruturam a **comunica√ß√£o** (trocas de mensagens) entre o **usu√°rio** e o **agente**.

---

### Tipos de Mensagens

* **`System`**: definem como o modelo deve se comportar, 
  * elas servem como instru√ß√µes persistentes, guiando cada intera√ß√£o subsequente.
* **`User`**: mensagens de instru√ß√£o do usu√°rio
* **`Assistant`**: mensagens de resposta do agente
  
Uma **conversa** consiste em **mensagens alternadas** entre um **Humano** (`user`) e um **LLM** (`assistant`).

Os `chat_templates` ajudam a manter o contexto, preservando o hist√≥rico de conversas entre o `usu√°rio` e o `assistente`. 
> Isso resulta em conversas mais coerentes e em m√∫ltiplos turnos.

Ao usar **Agentes**, a **Mensagem do Sistema** tamb√©m pode fornecer:

* informa√ß√µes sobre as **ferramentas** dispon√≠veis, 
* instru√ß√µes ao modelo sobre **como formatar as a√ß√µes** a serem tomadas e
* inclui **diretrizes** sobre como o **processo de pensamento** deve ser segmentado

---

### Modelos Base vs. Modelos de Instru√ß√£o

Outro ponto que precisamos entender √© a diferen√ßa entre um `Modelo Base` e um `Modelo de Instru√ß√£o`:

‚Ä¢	Um `Modelo Base` √© treinado com dados de texto brutos para prever o pr√≥ximo token.

‚Ä¢	Um `Modelo de Instru√ß√£o` √© **fine-tuned** (ajustado) especificamente para seguir instru√ß√µes e participar de conversas. 

Por exemplo, `SmolLM2-135M` √© um `modelo base`, enquanto `SmolLM2-135M-Instruct` √© sua variante **fine-tuned** para instru√ß√µes.

---

### Compreendendo os Modelos de Chat

Como cada **modelo de instru√ß√£o** utiliza diferentes `formatos de conversa√ß√£o` e `tokens especiais`, os **modelos de chat** s√£o implementados para garantir que formatemos o prompt corretamente, conforme esperado por cada modelo.

Em **transformers**, os `modelos de chat` incluem c√≥digo `Jinja2` que descreve como transformar a **lista de mensagens JSON do ChatML**, para que o modelo possa entender.

Essa estrutura ajuda a manter a consist√™ncia entre as intera√ß√µes e garante que o modelo responda adequadamente a diferentes tipos de entradas.

---

### Mensagens para Prompts

A maneira mais f√°cil de garantir que seu LLM receba uma conversa formatada corretamente √© usar o `chat_template` do `tokenizador` do modelo.

```python
messages = [
    {"role": "system", "content": "You are an AI assistant with access to various tools."},
    {"role": "user", "content": "Hi !"},
    {"role": "assistant", "content": "Hi human, what can help you with ?"},
]
```

Para converter a conversa anterior em um `prompt`, carregue o `tokenizador` e chame `apply_chat_template`:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM2-1.7B-Instruct")
rendered_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)      
```
O `rendered_prompt` retornado por esta fun√ß√£o agora est√° pronto para ser usado como entrada para o modelo que voc√™ escolheu!

Esta fun√ß√£o `apply_chat_template()` ser√° usada no backend da sua API quando voc√™ interagir com mensagens no formato `ChatML`.

Agora que vimos como os LLMs estruturam suas entradas por meio de `modelos de chat`, vamos explorar como os `Agentes` agem em seus ambientes.

Uma das principais maneiras de fazer isso √© usando `Ferramentas`, que __**estendem os recursos**__ de um modelo de IA para al√©m da gera√ß√£o de texto.

